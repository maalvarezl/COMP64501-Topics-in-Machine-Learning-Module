{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra and Linear Regression part 2\n",
    "\n",
    "\n",
    "In this second part of the Notebook, we will code linear regression using linear algebra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Example: Olympic Marathons\n",
    "\n",
    "We load in the Olympic marathon data. This is data of the olympic marathon times for the men's marathon from the first olympics in 1896 up until the London 2012 olympics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/maalvarezl/MLAI/master/Labs/datasets/olympic_marathon_men.csv', header=None, encoding= 'unicode_escape')\n",
    "x = np.array(data.iloc[:, 0].values).reshape(-1,1)\n",
    "y = np.array(data.iloc[:, 1].values).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Input Solution with Linear Algebra\n",
    "\n",
    "You've now seen how slow it can be to perform a coordinate ascent on a system. Another approach to solving the system (which is not always possible, particularly in *non-linear* systems) is to go directly to the minimum. To do this we need to introduce *linear algebra*. We will represent all our errors and functions in the form of linear algebra. \n",
    "\n",
    "As we mentioned above, linear algebra is just a shorthand for performing lots of multiplications and additions simultaneously. What does it have to do with our system then? Well the first thing to note is that the linear function we were trying to fit has the following form:\n",
    "$$\n",
    "f(x) = mx + c\n",
    "$$\n",
    "the classical form for a straight line. From a linear algebraic perspective we are looking for multiplications and additions. We are also looking to separate our parameters from our data. The data is the *givens* remember, in French the word is donnÃ©es literally translated means *givens* that's great, because we don't need to change the data, what we need to change are the parameters (or variables) of the model. In this function the data comes in through $x$, and the parameters are $m$ and $c$. \n",
    "\n",
    "What we'd like to create is a vector of parameters and a vector of data. Then we could represent the system with vectors that represent the data, and vectors that represent the parameters. \n",
    "\n",
    "We look to turn the multiplications and additions into a linear algebraic form, we have one multiplication ($m\\times c$) and one addition ($mx + c$). But we can turn this into a inner product by writing it in the following way,\n",
    "$$\n",
    "f(x) = m \\times x + c \\times 1,\n",
    "$$\n",
    "in other words we've extracted the unit value, from the offset, $c$. We can think of this unit value like an extra item of data, because it is always given to us, and it is always set to 1 (unlike regular data, which is likely to vary!). We can therefore write each input data location, $\\mathbf{x}$, as a vector\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} 1\\\\ x\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Now we choose to also turn our parameters into a vector. The parameter vector will be defined to contain \n",
    "$$\n",
    "\\mathbf{w} = \\begin{bmatrix} c \\\\ m\\end{bmatrix}\n",
    "$$\n",
    "because if we now take the inner product between these to vectors we recover\n",
    "$$\n",
    "\\mathbf{x}\\cdot\\mathbf{w} = 1 \\times c + x \\times m = mx + c\n",
    "$$\n",
    "In `numpy` we can define this vector as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vector w\n",
    "m = -0.4\n",
    "c = 80\n",
    "w = np.zeros(shape=(2, 1))\n",
    "w[0] = m\n",
    "w[1] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the equivalence between original operation and an operation in vector space. Whilst the notation here isn't a lot shorter, the beauty is that we will be able to add as many features as we like and still keep the same representation. In general, we are now moving to a system where each of our predictions is given by an inner product. When we want to represent a linear product in linear algebra, we tend to do it with the transpose operation, so since we have $\\mathbf{a}\\cdot\\mathbf{b} = \\mathbf{a}^\\top\\mathbf{b}$ we can write\n",
    "$$\n",
    "f(\\mathbf{x}_i) = \\mathbf{x}_i^\\top\\mathbf{w}.\n",
    "$$\n",
    "Where we've assumed that each data point, $\\mathbf{x}_i$, is now written by appending a 1 onto the original vector\n",
    "$$\n",
    "\\mathbf{x}_i = \n",
    "\\begin{bmatrix} \n",
    "1 \\\\\n",
    "x_i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Design Matrix\n",
    "\n",
    "We can do this for the entire data set to form a [*design matrix*](http://en.wikipedia.org/wiki/Design_matrix) $\\mathbf{X}$,\n",
    "\n",
    "$$\\mathbf{X} = \\begin{bmatrix} \n",
    "\\mathbf{x}_1^\\top \\\\\\ \n",
    "\\mathbf{x}_2^\\top \\\\\\ \n",
    "\\vdots \\\\\\\n",
    "\\mathbf{x}_n^\\top\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_1 \\\\\\\n",
    "1 & x_2 \\\\\\\n",
    "\\vdots & \\vdots \\\\\\\n",
    "1 & x_n \n",
    "\\end{bmatrix},$$\n",
    "\n",
    "which in `numpy` can be done with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones_like(x), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Objective with Linear Algebra\n",
    "\n",
    "When we think of the objective function, we can think of it as the errors where the error is defined in a similar way to what it was in Legendre's day $y_i - f(\\mathbf{x}_i)$, in statistics these errors are also sometimes called [*residuals*](http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics). So we can think as the objective and the prediction function as two separate parts, first we have,\n",
    "$$\n",
    "E(\\mathbf{w}) = \\sum_{i=1}^n (y_i - f(\\mathbf{x}_i; \\mathbf{w}))^2,\n",
    "$$\n",
    "where we've made the function $f(\\cdot)$'s dependence on the parameters $\\mathbf{w}$ explicit in this equation. Then we have the definition of the function itself,\n",
    "$$\n",
    "f(\\mathbf{x}_i; \\mathbf{w}) = \\mathbf{x}_i^\\top \\mathbf{w}.\n",
    "$$\n",
    "Let's look again at these two equations and see if we can identify any inner products. The first equation is a sum of squares, which is promising. Any sum of squares can be represented by an inner product,\n",
    "$$\n",
    "a = \\sum_{i=1}^{k} b^2_i = \\mathbf{b}^\\top\\mathbf{b},\n",
    "$$\n",
    "so if we wish to represent $E(\\mathbf{w})$ in this way, all we need to do is convert the sum operator to an inner product. We can get a vector from that sum operator by placing both $y_i$ and $f(\\mathbf{x}_i; \\mathbf{w})$ into vectors, which we do by defining \n",
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix}y_1\\\\y_2\\\\ \\vdots \\\\ y_n\\end{bmatrix}\n",
    "$$\n",
    "and defining\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{X}; \\mathbf{w}) = \\begin{bmatrix}f(\\mathbf{x}_1; \\mathbf{w})\\\\f(\\mathbf{x}_2; \\mathbf{w})\\\\ \\vdots \\\\ f(\\mathbf{x}_n; \\mathbf{w})\\end{bmatrix}.\n",
    "$$\n",
    "The second of these is actually a vector-valued function. This term may appear intimidating, but the idea is straightforward. A vector valued function is simply a vector whose elements are themselves defined as *functions*, i.e. it is a vector of functions, rather than a vector of scalars. The idea is so straightforward, that we are going to ignore it for the moment, and barely use it in the derivation. But it will reappear later when we introduce *basis functions*. So we will, for the moment, ignore the dependence of $\\mathbf{f}$ on $\\mathbf{w}$ and $\\mathbf{X}$ and simply summarise it by a vector of numbers\n",
    "$$\n",
    "\\mathbf{f} = \\begin{bmatrix}f_1\\\\f_2\\\\ \\vdots \\\\ f_n\\end{bmatrix}.\n",
    "$$\n",
    "This allows us to write our objective in the folowing, linear algebraic form,\n",
    "$$\n",
    "E(\\mathbf{w}) = (\\mathbf{y} - \\mathbf{f})^\\top(\\mathbf{y} - \\mathbf{f})\n",
    "$$\n",
    "from the rules of inner products.\n",
    "\n",
    "But what of our matrix $\\mathbf{X}$ of input data? At this point, we need to dust off [*matrix-vector multiplication*](http://en.wikipedia.org/wiki/Matrix_multiplication). Matrix multiplication is simply a convenient way of performing many inner products together, and it's exactly what we need to summarise the operation\n",
    "$$\n",
    "f_i = \\mathbf{x}_i^\\top\\mathbf{w}.\n",
    "$$\n",
    "This operation tells us that each element of the vector $\\mathbf{f}$ (our vector valued function) is given by an inner product between $\\mathbf{x}_i$ and $\\mathbf{w}$. In other words it is a series of inner products. Let's look at the definition of matrix multiplication, it takes the form\n",
    "$$\n",
    "\\mathbf{c} = \\mathbf{B}\\mathbf{a}\n",
    "$$\n",
    "where $\\mathbf{c}$ might be a $k$ dimensional vector (which we can intepret as a $k\\times 1$ dimensional matrix), and $\\mathbf{B}$ is a $k\\times k$ dimensional matrix and $\\mathbf{a}$ is a $k$ dimensional vector ($k\\times 1$ dimensional matrix). \n",
    "\n",
    "The result of this multiplication is of the form\n",
    "$$\n",
    "\\begin{bmatrix}c_1\\\\c_2 \\\\ \\vdots \\\\ c_k\\end{bmatrix} = \n",
    "\\begin{bmatrix} b_{1,1} & b_{1, 2} & \\dots & b_{1, k} \\\\\n",
    "b_{2, 1} & b_{2, 2} & \\dots & b_{2, k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{k, 1} & b_{k, 2} & \\dots & b_{k, k} \\end{bmatrix} \\begin{bmatrix}a_1\\\\a_2 \\\\ \\vdots\\\\ a_k\\end{bmatrix} = \\begin{bmatrix} b_{1, 1}a_1 + b_{1, 2}a_2 + \\dots + b_{1, k}a_k\\\\\n",
    "b_{2, 1}a_1 + b_{2, 2}a_2 + \\dots + b_{2, k}a_k \\\\ \n",
    "\\vdots\\\\ \n",
    "b_{k, 1}a_1 + b_{k, 2}a_2 + \\dots + b_{k, k}a_k\\end{bmatrix}\n",
    "$$\n",
    "so we see that each element of the result in $\\mathbf{c}$ is simply the inner product between each *row* of $\\mathbf{B}$ and the vector $\\mathbf{a}$. Because we have defined each element of $\\mathbf{f}$ to be given by the inner product between each *row* of the design matrix and the vector $\\mathbf{w}$ we now can write the full operation in one matrix multiplication,\n",
    "$$\n",
    "\\mathbf{f} = \\mathbf{X}\\mathbf{w}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.dot(X, w) # np.dot does matrix multiplication in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this result with our objective function,\n",
    "$$\n",
    "E(\\mathbf{w}) = (\\mathbf{y} - \\mathbf{f})^\\top(\\mathbf{y} - \\mathbf{f})\n",
    "$$\n",
    "we find we have defined the *model* with two equations. One equation tells us the form of our predictive function and how it depends on its parameters, the other tells us the form of our objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error function is: [[6.61507248e+11]]\n"
     ]
    }
   ],
   "source": [
    "resid = (y-f)\n",
    "E = np.dot(resid.T, resid) # matrix multiplication on a single vector is equivalent to a dot product.\n",
    "print(\"Error function is:\", E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Optimisation\n",
    "\n",
    "Our *model* has now been defined with two equations, the prediction function and the objective function. Next we will use multivariate calculus to define an *algorithm* to fit the model. The separation between model and algorithm is important and is often overlooked. Our model contains a function that shows how it will be used for prediction, and a function that describes the objective function we need to optimise to obtain a good set of parameters. \n",
    "\n",
    "The linear regression model we have described is still the same as the one we fitted above with a coordinate ascent algorithm. We have only played with the notation to obtain the same model in a matrix and vector notation. However, we will now fit this model with a different algorithm, one that is much faster. It is such a widely used algorithm that from the end user's perspective it doesn't even look like an algorithm, it just appears to be a single operation (or function). However, underneath the computer calls an algorithm to find the solution. Further, the algorithm we obtain is very widely used, and because of this it turns out to be highly optimised.\n",
    "\n",
    "Once again we are going to try and find the stationary points of our objective by finding the *stationary points*. However, the stationary points of a multivariate function, are a little bit more complext to find. Once again we need to find the point at which the derivative is zero, but now we need to use  *multivariate calculus* to find it. This involves learning a few additional rules of differentiation (that allow you to do the derivatives of a function with respect to  vector), but in the end it makes things quite a bit easier. We define vectorial derivatives as follows,\n",
    "$$\n",
    "\\frac{\\text{d}E(\\mathbf{w})}{\\text{d}\\mathbf{w}} = \\begin{bmatrix}\\frac{\\partial E(\\mathbf{w})}{\\partial w_1}\\\\\\frac{\\partial E(\\mathbf{w})}{\\partial w_2}\\end{bmatrix}.\n",
    "$$\n",
    "where $\\frac{\\partial E(\\mathbf{w})}{\\partial w_1}$ is the [partial derivative](http://en.wikipedia.org/wiki/Partial_derivative) of the error function with respect to $w_1$.\n",
    "\n",
    "Differentiation through multiplications and additions is relatively straightforward, and since linear algebra is just multiplication and addition, then its rules of diffentiation are quite straightforward too, but slightly more complex than regular derivatives. \n",
    "\n",
    "### Matrix Differentiation\n",
    "\n",
    "We will need two rules of differentiation. The first is diffentiation of an inner product. By remebering that the inner product is made up of multiplication and addition, we can hope that its derivative is quite straightforward, and so it proves to be. We can start by thinking about the definition of the inner product,\n",
    "$$\n",
    "\\mathbf{a}^\\top\\mathbf{z} = \\sum_{i} a_i z_i,\n",
    "$$\n",
    "which if we were to take the derivative with respect to $z_k$ would simply return the gradient of the one term in the sum for which the derivative was non zero, that of $a_k$, so we know that \n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}z_k} \\mathbf{a}^\\top \\mathbf{z} = a_k\n",
    "$$\n",
    "and by our definition of multivariate derivatives we can simply stack all the partial derivatives of this form in a vector to obtain the result that\n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}} \\mathbf{a}^\\top \\mathbf{z} = \\mathbf{a}.\n",
    "$$\n",
    "The second rule that's required is differentiation of a 'matrix quadratic'. A scalar quadratic in $z$ with coefficient $c$ has the form $cz^2$. If $\\mathbf{z}$ is a $k\\times 1$ vector and $\\mathbf{C}$ is a $k \\times k$ *matrix* of coefficients then the matrix quadratic form is written as $\\mathbf{z}^\\top \\mathbf{C}\\mathbf{z}$, which is itself a *scalar* quantity, but it is a function of a *vector*. \n",
    "\n",
    "#### Matching Dimensions in Matrix Multiplications\n",
    "\n",
    "There's a trick for telling that it's a scalar result. When you are doing maths with matrices, it's always worth pausing to perform a quick sanity check on the dimensions. Matrix multplication only works when the dimensions match. To be precise, the 'inner' dimension of the matrix must match. What is the inner dimension. If we multiply two matrices $\\mathbf{A}$ and $\\mathbf{B}$, the first of which has $k$ rows and $\\ell$ columns and the second of which has $p$ rows and $q$ columns, then we can check whether the multiplication works by writing the dimensionalities next to each other,\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{B} \\rightarrow (k \\times \\underbrace{\\ell)(p}_\\text{inner dimensions} \\times q) \\rightarrow (k\\times q).\n",
    "$$\n",
    "The inner dimensions are the two inside dimensions, $\\ell$ and $p$. The multiplication will only work if $\\ell=p$. The result of the multiplication will then be a $k\\times q$ matrix: this dimensionality comes from the 'outer dimensions'. Note that matrix multiplication is not [*commutative*](http://en.wikipedia.org/wiki/Commutative_property). And if you change the order of the multiplication, \n",
    "$$\n",
    "\\mathbf{B} \\mathbf{A} \\rightarrow (\\ell \\times \\underbrace{k)(q}_\\text{inner dimensions} \\times p) \\rightarrow (\\ell \\times p).\n",
    "$$\n",
    "firstly it may no longer even work, because now the condition is that $k=q$, and secondly the result could be of a different dimensionality. An exception is if the matrices are square matrices (e.g. same number of rows as columns) and they are both *symmetric*. A symmetric matrix is one for which $\\mathbf{A}=\\mathbf{A}^\\top$, or equivalently, $a_{i,j} = a_{j,i}$ for all $i$ and $j$.  \n",
    "\n",
    "You will need to get used to working with matrices and vectors applying and developing new machine learning techniques. You should have come across them before, but you may not have used them as extensively as we will now do in this course. You should get used to using this trick to check your work and ensure you know what the dimension of an output matrix should be. For our matrix quadratic form, it turns out that we can see it as a special type of inner product.\n",
    "$$\n",
    "\\mathbf{z}^\\top\\mathbf{C}\\mathbf{z} \\rightarrow (1\\times \\underbrace{k) (k}_\\text{inner dimensions}\\times k) (k\\times 1) \\rightarrow \\mathbf{b}^\\top\\mathbf{z}\n",
    "$$\n",
    "where $\\mathbf{b} = \\mathbf{C}\\mathbf{z}$ so therefore the result is a scalar,\n",
    "$$\n",
    "\\mathbf{b}^\\top\\mathbf{z} \\rightarrow (1\\times \\underbrace{k) (k}_\\text{inner dimensions}\\times 1) \\rightarrow (1\\times 1)\n",
    "$$\n",
    "where a $(1\\times 1)$ matrix is recognised as a scalar.\n",
    "\n",
    "This implies that we should be able to differentiate this form, and indeed the rule for its differentiation is slightly more complex than the inner product, but still quite simple,\n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}} \\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}= \\mathbf{C}\\mathbf{z} + \\mathbf{C}^\\top \\mathbf{z}.\n",
    "$$\n",
    "Note that in the special case where $\\mathbf{C}$ is symmetric then we have $\\mathbf{C} = \\mathbf{C}^\\top$ and the derivative simplifies to \n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}} \\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}= 2\\mathbf{C}\\mathbf{z}.\n",
    "$$\n",
    "### Differentiating the Objective\n",
    "\n",
    "First, we need to compute the full objective by substituting our prediction function into the objective function to obtain the objective in terms of $\\mathbf{w}$. Doing this we obtain\n",
    "$$\n",
    "E(\\mathbf{w})= (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top (\\mathbf{y} - \\mathbf{X}\\mathbf{w}).\n",
    "$$\n",
    "We now need to differentiate this *quadratic form* to find the minimum. We differentiate with respect to the *vector* $\\mathbf{w}$. But before we do that, we'll expand the brackets in the quadratic form to obtain a series of scalar terms. The rules for bracket expansion across the vectors are similar to those for the scalar system giving,\n",
    "$$\n",
    "(\\mathbf{a} - \\mathbf{b})^\\top (\\mathbf{c} - \\mathbf{d}) = \\mathbf{a}^\\top \\mathbf{c} - \\mathbf{a}^\\top \\mathbf{d} - \\mathbf{b}^\\top \\mathbf{c} + \\mathbf{b}^\\top \\mathbf{d}\n",
    "$$\n",
    "which substituting for $\\mathbf{a} = \\mathbf{c} = \\mathbf{y}$ and $\\mathbf{b}=\\mathbf{d} = \\mathbf{X}\\mathbf{w}$ gives\n",
    "$$\n",
    "E(\\mathbf{w})= \\mathbf{y}^\\top\\mathbf{y} - 2\\mathbf{y}^\\top\\mathbf{X}\\mathbf{w} + \\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "where we used the fact that $\\mathbf{y}^\\top\\mathbf{X}\\mathbf{w}= \\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{y}$. Now we can use our rules of differentiation to compute the derivative of this form, which is,\n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{w}}E(\\mathbf{w})=- 2\\mathbf{X}^\\top \\mathbf{y} + 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w},\n",
    "$$\n",
    "where we have exploited the fact that $\\mathbf{X}^\\top\\mathbf{X}$ is symmetric to obtain this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Use the equivalence between our vector and our matrix formulations of linear regression, alongside our definition of vector derivates, to match the gradients we've computed directly for $\\frac{\\text{d}E(c, m)}{\\text{d}c}$ and $\\frac{\\text{d}E(c, m)}{\\text{d}m}$ to those for $\\frac{\\text{d}E(\\mathbf{w})}{\\text{d}\\mathbf{w}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer\n",
    "\n",
    "*Double click to edit this cell and write your answer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Equation for Global Optimum\n",
    "\n",
    "Once again, we need to find the minimum of our objective function. Using our likelihood for multiple input regression we can now minimize for our parameter vector $\\mathbf{w}$. Firstly, just as in the single input case, we seek stationary points by finding parameter vectors that solve for when the gradients are zero,\n",
    "$$\n",
    "\\mathbf{0}=- 2\\mathbf{X}^\\top \\mathbf{y} + 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w},\n",
    "$$\n",
    "where $\\mathbf{0}$ is a *vector* of zeros. Rearranging this equation we find the solution to be\n",
    "$$\n",
    "\\mathbf{w} = \\left[\\mathbf{X}^\\top \\mathbf{X}\\right]^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$ \n",
    "where $\\mathbf{A}^{-1}$ denotes [*matrix inverse*](http://en.wikipedia.org/wiki/Invertible_matrix).\n",
    "\n",
    "### Solving the Multivariate System\n",
    "\n",
    "The solution for $\\mathbf{w}$ is given in terms of a matrix inverse, but computation of a matrix inverse requires, in itself, an algorithm to resolve it. You'll know this if you had to invert, by hand, a $3\\times 3$ matrix in high school. From a numerical stability perspective, it is also best not to compute the matrix inverse directly, but rather to ask the computer to *solve* the  system of linear equations given by\n",
    "$$\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} = \\mathbf{X}^\\top\\mathbf{y}$$\n",
    "for $\\mathbf{w}$. This can be done in `numpy` using the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Solve a linear matrix equation, or system of linear scalar equations.\n",
      "\n",
      "Computes the \"exact\" solution, `x`, of the well-determined, i.e., full\n",
      "rank, linear matrix equation `ax = b`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a : (..., M, M) array_like\n",
      "    Coefficient matrix.\n",
      "b : {(..., M,), (..., M, K)}, array_like\n",
      "    Ordinate or \"dependent variable\" values.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "x : {(..., M,), (..., M, K)} ndarray\n",
      "    Solution to the system a x = b.  Returned shape is identical to `b`.\n",
      "\n",
      "Raises\n",
      "------\n",
      "LinAlgError\n",
      "    If `a` is singular or not square.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "scipy.linalg.solve : Similar function in SciPy.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      ".. versionadded:: 1.8.0\n",
      "\n",
      "Broadcasting rules apply, see the `numpy.linalg` documentation for\n",
      "details.\n",
      "\n",
      "The solutions are computed using LAPACK routine ``_gesv``.\n",
      "\n",
      "`a` must be square and of full-rank, i.e., all rows (or, equivalently,\n",
      "columns) must be linearly independent; if either is not true, use\n",
      "`lstsq` for the least-squares best \"solution\" of the\n",
      "system/equation.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,\n",
      "       FL, Academic Press, Inc., 1980, pg. 22.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Solve the system of equations ``x0 + 2 * x1 = 1`` and ``3 * x0 + 5 * x1 = 2``:\n",
      "\n",
      ">>> a = np.array([[1, 2], [3, 5]])\n",
      ">>> b = np.array([1, 2])\n",
      ">>> x = np.linalg.solve(a, b)\n",
      ">>> x\n",
      "array([-1.,  1.])\n",
      "\n",
      "Check that the solution is correct:\n",
      "\n",
      ">>> np.allclose(np.dot(a, x), b)\n",
      "True\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\ssummerton\\envs\\comp24112\\lib\\site-packages\\numpy\\linalg\\linalg.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "np.linalg.solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can obtain the solution using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.88952457e+01]\n",
      " [-1.29806477e-02]]\n"
     ]
    }
   ],
   "source": [
    "w = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map it back to the liner regression and plot the fit as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01298065]\n",
      "[28.89524568]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16c6c5d4bb0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import pylab as plt\n",
    "m = w[1]; c=w[0]\n",
    "x_test = np.linspace(1890, 2020, 130)[:, None]\n",
    "f_test = m*x_test + c\n",
    "print(m)\n",
    "print(c)\n",
    "plt.plot(x_test, f_test, 'b-')\n",
    "plt.plot(x, y, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "\n",
    "A major advantage of the new system is that we can build a linear regression on a multivariate system. The matrix calculus didn't specify what the length of the vector $\\mathbf{x}$ should be, or equivalently the size of the design matrix. \n",
    "\n",
    "### Movie Body Count Data\n",
    "\n",
    "Let's load back in the movie body count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('https://raw.githubusercontent.com/maalvarezl/MLAI/master/Labs/datasets/film-death-counts-Python.csv',encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of the features we've been provided with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Film, Year, Body_Count, MPAA_Rating, Genre, Director, Actors, Length_Minutes, IMDB_Rating\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(movies.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a design matrix based on the numeric features: year, Body_Count, Length_Minutes in an effort to predict the rating. We build the design matrix as follows:\n",
    "\n",
    "## Relation to Single Input System\n",
    "\n",
    "Bias as an additional feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features = ['Year', 'Body_Count', 'Length_Minutes']\n",
    "X = movies.loc[:, select_features]\n",
    "X['Eins'] = 1 # add a column for the offset\n",
    "y = movies[['IMDB_Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform a linear regression. But this time, we will create a pandas data frame for the result so we can store it in a form that we can visualise easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regression_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>-0.016280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body_Count</th>\n",
       "      <td>-0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length_Minutes</th>\n",
       "      <td>0.025386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eins</th>\n",
       "      <td>36.508363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                regression_coefficient\n",
       "Year                         -0.016280\n",
       "Body_Count                   -0.000995\n",
       "Length_Minutes                0.025386\n",
       "Eins                         36.508363"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "w = pd.DataFrame(data=np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y)),  # solve linear regression here\n",
    "                 index = X.columns,  # columns of X become rows of w\n",
    "                 columns=['regression_coefficient']) # the column of X is the value of regression coefficient\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the residuals to see how good our estimates are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'IMDB_Rating'}>]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(y - np.dot(X, w)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shows our model *hasn't* yet done a great job of representation, because the spread of values is large. We can check what the rating is dominated by in terms of regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regression_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>-0.016280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body_Count</th>\n",
       "      <td>-0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length_Minutes</th>\n",
       "      <td>0.025386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eins</th>\n",
       "      <td>36.508363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                regression_coefficient\n",
       "Year                         -0.016280\n",
       "Body_Count                   -0.000995\n",
       "Length_Minutes                0.025386\n",
       "Eins                         36.508363"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have to be a little careful about interpretation because our input values live on different scales, however it looks like we are dominated by the bias, with a small negative effect for later films (but bear in mind the years are large, so this effect is probably larger than it looks) and a positive effect for length. So it looks like long earlier films generally do better, but the residuals are so high that we probably haven't modelled the system very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution with QR Decomposition\n",
    "\n",
    "Performing a solve instead of a matrix inverse is the more numerically stable approach, but we can do even better. A [QR-decomposition](http://en.wikipedia.org/wiki/QR_decomposition) of a matrix factorises it into a matrix which is an orthogonal matrix $\\mathbf{Q}$, so that $\\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{I}$. And a matrix which is upper triangular, $\\mathbf{R}$. \n",
    "$$\n",
    "\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "$$\n",
    "(\\mathbf{Q}\\mathbf{R})^\\top (\\mathbf{Q}\\mathbf{R})\\boldsymbol{\\beta} = (\\mathbf{Q}\\mathbf{R})^\\top \\mathbf{y}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{R}^\\top (\\mathbf{Q}^\\top \\mathbf{Q}) \\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{R}^\\top \\mathbf{Q}^\\top \\mathbf{y}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{R}^\\top \\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{R}^\\top \\mathbf{Q}^\\top \\mathbf{y}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{Q}^\\top \\mathbf{y}\n",
    "$$\n",
    "This is a more numerically stable solution because it removes the need to compute $\\mathbf{X}^\\top\\mathbf{X}$ as an intermediate. Computing $\\mathbf{X}^\\top\\mathbf{X}$ is a bad idea because it involves squaring all the elements of $\\mathbf{X}$ and thereby potentially reducing the numerical precision with which we can represent the solution. Operating on $\\mathbf{X}$ directly preserves the numerical precision of the model.\n",
    "\n",
    "This can be more particularly seen when we begin to work with *basis functions* in the next week. Some systems that can be resolved with the QR decomposition can not be resolved by using solve directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>-0.016280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body_Count</th>\n",
       "      <td>-0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length_Minutes</th>\n",
       "      <td>0.025386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eins</th>\n",
       "      <td>36.508363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0\n",
       "Year            -0.016280\n",
       "Body_Count      -0.000995\n",
       "Length_Minutes   0.025386\n",
       "Eins            36.508363"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.linalg as linalg \n",
    "Q, R = np.linalg.qr(X)\n",
    "w = linalg.solve_triangular(R, np.dot(Q.T, y)) \n",
    "w = pd.DataFrame(w, index=X.columns)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial code: it uses pods.notebook.display_prediction, but with a minor modification to \n",
    "# allow the use of ipywidgets\n",
    "from ipywidgets import *\n",
    "def display_prediction(basis, num_basis=4, wlim=(-1.,1.), fig=None, ax=None, xlim=None, ylim=None, num_points=1000, offset=0.0, **kwargs):\n",
    "    \"\"\"Interactive widget for displaying a prediction function based on summing separate basis functions.\n",
    "    :param basis: a function handle that calls the basis functions.\n",
    "    :type basis: function handle.\n",
    "    :param xlim: limits of the x axis to use.\n",
    "    :param ylim: limits of the y axis to use.\n",
    "    :param wlim: limits for the basis function weights.\"\"\"\n",
    "\n",
    "    #import numpy as np\n",
    "    #import pylab as plt\n",
    "\n",
    "    if fig is not None:\n",
    "        if ax is None:\n",
    "            ax = fig.gca()\n",
    "\n",
    "    if xlim is None:\n",
    "        if ax is not None:\n",
    "            xlim = ax.get_xlim()\n",
    "        else:\n",
    "            xlim = (-2., 2.)\n",
    "    if ylim is None:\n",
    "        if ax is not None:\n",
    "            ylim = ax.get_ylim()\n",
    "        else:\n",
    "            ylim = (-1., 1.)\n",
    "\n",
    "    # initialise X and set up W arguments.\n",
    "    x = np.zeros((num_points, 1))\n",
    "    x[:, 0] = np.linspace(xlim[0], xlim[1], num_points)\n",
    "    param_args = {}\n",
    "    for i in range(num_basis):\n",
    "        lim = list(wlim)\n",
    "        if i ==0:\n",
    "            lim[0] += offset\n",
    "            lim[1] += offset\n",
    "        param_args['w_' + str(i)] = tuple(lim)\n",
    "\n",
    "    # helper function for making basis prediction.\n",
    "    def predict_basis(w, basis, x, num_basis, **kwargs):\n",
    "        Phi = basis(x, num_basis, **kwargs)\n",
    "        f = np.dot(Phi, w)\n",
    "        return f, Phi\n",
    "    \n",
    "    if type(basis) is dict:\n",
    "        use_basis = basis[list(basis.keys())[0]]\n",
    "    else:\n",
    "        use_basis = basis\n",
    "    f, Phi = predict_basis(np.zeros((num_basis, 1)),\n",
    "                           use_basis, x, num_basis,\n",
    "                           **kwargs)\n",
    "    if fig is None:\n",
    "        fig, ax=plt.subplots(figsize=(12,4))\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "    predline = ax.plot(x, f, linewidth=2)[0]\n",
    "    basislines = []\n",
    "    for i in range(num_basis):\n",
    "        basislines.append(ax.plot(x, Phi[:, i], 'r')[0])\n",
    "\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    def generate_function(basis, num_basis, predline, basislines, basis_args, display_basis, offset, **kwargs):\n",
    "        w = np.zeros((num_basis, 1))\n",
    "        for i in range(num_basis):\n",
    "            w[i] = kwargs['w_'+ str(i)]\n",
    "        f, Phi = predict_basis(w, basis, x, num_basis, **basis_args)\n",
    "        predline.set_xdata(x[:, 0])\n",
    "        predline.set_ydata(f)\n",
    "        for i in range(num_basis):\n",
    "            basislines[i].set_xdata(x[:, 0])\n",
    "            basislines[i].set_ydata(Phi[:, i])\n",
    "\n",
    "        if display_basis:\n",
    "            for i in range(num_basis):\n",
    "                basislines[i].set_alpha(1) # make visible\n",
    "        else:\n",
    "            for i in range(num_basis):\n",
    "                basislines[i].set_alpha(0) \n",
    "        display(fig)\n",
    "    if type(basis) is not dict:\n",
    "        basis = fixed(basis)\n",
    "\n",
    "    plt.close(fig)\n",
    "    interact(generate_function, \n",
    "             basis=basis,\n",
    "             num_basis=fixed(num_basis),\n",
    "             predline=fixed(predline),\n",
    "             basislines=fixed(basislines),\n",
    "             basis_args=fixed(kwargs),\n",
    "             offset = fixed(offset),\n",
    "             display_basis = False,\n",
    "             **param_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Functions\n",
    "\n",
    "We've now seen how we may perform linear regression. Now, we are going to consider how we can perform *non-linear* regression. However, before we get into the details of how to do that we first need to consider in what ways the regression can be non-linear. \n",
    "\n",
    "Multivariate linear regression allows us to build models that take many features into account when making our prediction. In this session we are going to introduce *basis functions*. The term seems complicated, but they are actually based on rather a simple idea. If we are doing a multivariate linear regression, we get extra features that *might* help us predict our required response variable (or target value), $y$. But what if we only have one input value? We can actually artificially generate more input values with basis functions.\n",
    "\n",
    "### Non-linear in the Inputs\n",
    "\n",
    "When we refer to non-linear regression, we are normally referring to whether the regression is non-linear in the input space, or non-linear in the *covariates*. The covariates are the observations that move with the target (or *response*) variable. In our notation we have been using $\\mathbf{x}_i$ to represent a vector of the covariates associated with the $i$th observation. The coresponding response variable is $y_i$. If a model is non-linear in the inputs, it means that there is a non-linear function between the inputs and the response variable. Linear functions are functions that only involve multiplication and addition, in other words they can be represented through *linear algebra*. Linear regression involves assuming that a function takes the form\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}\n",
    "$$\n",
    "where $\\mathbf{w}$ are our regression weights. A very easy way to make the linear regression non-linear is to introduce non-linear functions. When we are introducing non-linear regression these functions are known as *basis functions*.\n",
    "\n",
    "### Basis Functions\n",
    "\n",
    "Here's the idea, instead of working directly on the original input space, $\\mathbf{x}$, we build models in a new space, $\\boldsymbol{\\phi}(\\mathbf{x})$ where $\\boldsymbol{\\phi}(\\cdot)$ is a *vector valued* function that is defined on the space $\\mathbf{x}$. \n",
    "\n",
    "Remember, that a vector valued function is just a vector that contains functions instead of values. Here's an example for a one dimensional input space, $x$, being projected to a *quadratic* basis. First we consider each basis function in turn, we can think of the elements of our vector as being indexed so that we have\n",
    "\\begin{align*}\n",
    "\\phi_1(x) = 1, \\\\\n",
    "\\phi_2(x) = x, \\\\\n",
    "\\phi_3(x) = x^2.\n",
    "\\end{align*}\n",
    "Now we can consider them together by placing them in a vector,\n",
    "$$\n",
    "\\boldsymbol{\\phi}(x) = \\begin{bmatrix} 1\\\\ x \\\\ x^2\\end{bmatrix}.\n",
    "$$\n",
    "This is the idea of the vector valued function, we have simply collected the different functions together in the same vector making them notationally easier to deal with in our mathematics. \n",
    "\n",
    "When we consider the vector valued function for each data point, then we place all the data into a matrix. The result is a matrix valued function,\n",
    "$$\n",
    "\\boldsymbol{\\Phi}(\\mathbf{x}) = \n",
    "\\begin{bmatrix} 1 & x_1 & x_1^2 \\\\\n",
    "1 & x_2 & x_2^2\\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_n & x_n^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where we are still in the one dimensional input setting so $\\mathbf{x}$ here represents a vector of our inputs with $n$ elements. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial basis extends the quadratic basis to arbitrary degree, so we might define the $j$th basis function associated with the model as\n",
    "$$\n",
    "\\phi_j(x_i) = x_i^j\n",
    "$$\n",
    "which can be implemented as a function in code as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, num_basis=4, data_limits=[-1., 1.]):\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:, i:i+1] = x**i\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid in understanding how a basis works, we've provided you with a small interactive tool for exploring this polynomial basis. The tool can be summoned with the following command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** if this cell doesn't show sliders for you, the you may need to save your notebook and shut down the jupyter server in the terminal then run `jupyter nbextension enable --py widgetsnbextension`, the resart you notebook and run all the cells up to here. \n",
    "\n",
    "See [this page](https://stackoverflow.com/questions/36351109/ipython-notebook-ipywidgets-does-not-show) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636b8329fb6d43cd902baa88d4ee946e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='display_basis'), FloatSlider(value=0.0, description='â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prediction(basis=polynomial, num_basis=4, ylim=[-3.,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try moving the sliders around to change the weight of each basis function. Click the control box `display_basis` to show the underlying basis functions (in red). The prediction function is shown in a thick blue line. *Warning* the sliders aren't presented quite in the correct order. `w_0` is associated with the bias, `w_1` is the linear term, `w_2` the quadratic and here (because we have four basis functions) we have `w_3` for the *cubic* term. So the subscript of the weight parameter is always associated with the corresponding polynomial's degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Try increasing the number of basis functions (thereby increasing the *degree* of the resulting polynomial). Describe what you see as you increase number of basis up to 10. Is it easy to change the function in intuitive ways? For example, if you want to manually set the weights $\\mathbf{w}$ for a particular training dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 2 Answer code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 Answer\n",
    "\n",
    "*Double click to edit this cell and write your answer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting to Data\n",
    "\n",
    "Now we are going to consider how these basis functions can be adjusted to fit to a particular data set. We return to the olympic marathon data. First we will scale the output of the data to be zero mean and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/maalvarezl/MLAI/master/Labs/datasets/olympic_marathon_men.csv', header=None, encoding= 'unicode_escape')\n",
    "x = np.array(data.iloc[:, 0].values).reshape(-1,1)\n",
    "y = np.array(data.iloc[:, 1].values).reshape(-1,1)\n",
    "y -= y.mean()\n",
    "y /= y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Now we are going to redefine our polynomial basis. Have a careful look at the operations we perform on `x` to create `z`. We use `z` in the polynomial computation. What are we doing to the inputs? Why do you think we are changing `x` in this manner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, num_basis=4, data_limits=[-1., 1.]):\n",
    "    \"Polynomial basis\"\n",
    "    centre = data_limits[0]/2. + data_limits[1]/2.\n",
    "    span = data_limits[1] - data_limits[0]\n",
    "    z = x - centre\n",
    "    z = 2*z/span\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:, i:i+1] = z**i\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 Answer\n",
    "\n",
    "*Double click to edit this cell and write your answer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We like to make use of *design* matrices for our data. Design matrices, as you will recall, involve placing the data points into rows of the matrix and data features into the columns of the matrix. By convention, we are referencing a vector with a bold lower case letter, and a matrix with a bold upper case letter. The design matrix is therefore given by\n",
    "$$\n",
    "\\boldsymbol{\\Phi} = \\begin{bmatrix} 1 & \\mathbf{x} & \\mathbf{x}^2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### Non-linear but linear in the Parameters\n",
    "\n",
    "One rather nice aspect of our model is that whilst it is non-linear in the inputs, it is still linear in the parameters $\\mathbf{w}$. This means that our derivations from before continue to operate to allow us to work with this model. In fact, although this is a non-linear regression it is still known as a *linear model* because it is linear in the parameters, \n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "where the vector $\\mathbf{x}$ appears inside the basis functions, making our result, $f(\\mathbf{x})$ non-linear in the inputs, but $\\mathbf{w}$ appears outside our basis function, making our result *linear* in the parameters. In practice, our basis function itself may contain its own set of parameters,\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}; \\boldsymbol{\\theta}),\n",
    "$$\n",
    "that we've denoted here as $\\boldsymbol{\\theta}$. If these parameters appear inside the basis function then our model is *non-linear* in these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "For the following prediction functions state whether the model is linear in the inputs, the parameters or both.\n",
    "\n",
    "(a) $f(x) = w_1x_1 + w_2$\n",
    "\n",
    "(b) $f(x) = w_1\\exp(x_1) + w_2x_2 + w_3$\n",
    "\n",
    "(c) $f(x) = \\log(x_1^{w_1}) + w_2x_2^2 + w_3$\n",
    "\n",
    "(d) $f(x) = \\exp(-\\sum_i(x_i - w_i)^2)$\n",
    "\n",
    "(e) $f(x) = \\exp(-\\mathbf{w}^\\top \\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Answer\n",
    "\n",
    "*Double click to edit this cell and write your answer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model Yourself\n",
    "\n",
    "You now have everything you need to fit a non-linear (in the inputs) basis function model to the marathon data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "For this question, use the polynomial basis function. Compute the design matrix on the covariates (or input data), `x`. Use the design matrix and the response variable `y` to solve the following linear system for the model parameters `w`.\n",
    "$$\n",
    "\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi}\\mathbf{w} = \\boldsymbol{\\Phi}^\\top \\mathbf{y}\n",
    "$$\n",
    "Compute the corresponding error on the training data. How does it compare to the error you were able to achieve using the linear model with respect to the inputs? Plot the form of your prediction function from the least squares estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5 Answer Code\n",
    "# Write code for you answer to this question in this box"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP24112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
